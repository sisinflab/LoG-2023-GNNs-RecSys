{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyNCQPT/Mw2WSRwTnzeE8d+j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# [Hands-on #2] Reproducibility\n","\n","\n","<div>\n","  <img src=\"https://logconference.org/post/announcement/featured.png\" alt=\"Poliba\" width=\"100\">\n","</div>\n","\n","This is the Google Colab notebook for the hands-on session #2 of the tutorial: \"_[Graph Neural Networks for Recommendation: Reproducibility, Graph Topology, and Node Representation](https://sisinflab.github.io/tutorial-gnns-recsys-log2023/)_\" presented at the [2nd Learning on Graphs Conference](https://logconference.org/) (LoG 2023) -- November, 30 (Online).\n","\n","Credits:\n","- Daniele Malitesta (daniele.malitesta@poliba.it)\n","- Claudio Pomo (claudio.pomo@poliba.it)\n","- Tommaso Di Noia (tommaso.dinoia@poliba.it)\n","\n","<div>\n","  <img src=\"https://www.poliba.it/sites/default/files/logo_5.png\" alt=\"Poliba\" width=\"200\">\n","  <img src=\"https://swot.sisinflab.poliba.it/img/logo-sisinflab.png\" alt=\"SisInfLab\" width=\"200\">\n","</div>"],"metadata":{"id":"3KiLoN1pcxP4"}},{"cell_type":"markdown","source":["## Clone the repository\n","\n","First, let's clone the repository from GitHub..."],"metadata":{"id":"eluRHZzVR_Au"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXFH1E8xL-KC"},"outputs":[],"source":["!git clone https://github.com/sisinflab/Multimodal-RSs-Reproducibility.git"]},{"cell_type":"markdown","source":["## Set up the environment\n","\n","Second, let's set up the environment with the needed (extra) pip packages and the environment variables to ensure reproducibility!"],"metadata":{"id":"mHW69OKpSGoK"}},{"cell_type":"code","source":["%cd Multimodal-RSs-Reproducibility/\n","%env PYTHONPATH=.\n","%env CUBLAS_WORKSPACE_CONFIG=:16:8\n","!pip install -r requirements_torch_geometric_colab.txt"],"metadata":{"id":"qubulQo1K6SP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check if GPU is available\n","\n","Then, let's check if the GPU is available:"],"metadata":{"id":"Sk8_rR50VFe6"}},{"cell_type":"code","source":["!nvidia-smi\n","!nvcc --version"],"metadata":{"id":"cq-j--VVVQ3U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configure the experiments\n","Let's set the hyper-parameters for the model to be trained and tested. We begin with LightGCN in a modified version which adopts multimodal features (we call it LightGCNM). We train and evaluate it on Amazon Office."],"metadata":{"id":"HqcVn3KaRUkg"}},{"cell_type":"code","source":["import yaml\n","\n","config_filename = 'hands-on-2-log_2023.yml'\n","config = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train.txt',\n","      'validation_path': '../data/{0}/val.txt',\n","      'test_path': '../data/{0}/test.txt',\n","      'side_information': [\n","        {\n","            'dataloader': 'VisualAttribute',\n","            'visual_features': '../data/{0}/image_feat'\n","        },\n","        {\n","            'dataloader': 'TextualAttribute',\n","            'textual_features': '../data/{0}/text_feat'\n","        }\n","      ]\n","    },\n","    'dataset': 'office',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'nDCG']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.LightGCNM': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'save_recs': False,\n","          'validation_rate': 10,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'epochs': 200,\n","        'batch_size': 1024,\n","        'factors': 64,\n","        'lr': 0.005,\n","        'l_w': 1e-5,\n","        'n_layers': 1,\n","        'normalize': True,\n","        'aggregation': 'mean',\n","        'modalities': \"('visual','textual')\",\n","        'loaders': \"('VisualAttribute','TextualAttribute')\",\n","        'seed': 123\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}', 'w') as file:\n","    documents = yaml.dump(config, file)"],"metadata":{"id":"Q2OuzsQpgXOK","executionInfo":{"status":"ok","timestamp":1701244048648,"user_tz":-60,"elapsed":232,"user":{"displayName":"Daniele Malitesta","userId":"17706482384632116882"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Download the multimodal recommendation dataset\n","\n","Before we launch the experiments, we need to download the multimodal dataset (i.e., Amazon Office).\n","\n","**[DISCLAIMER]** You can find the original version of Amazon Office at this [link](https://cseweb.ucsd.edu/~jmcauley/datasets/amazon/links.html). We downloaded and processed the multimodal features following the official [GitHub repository](https://github.com/CRIPAC-DIG/LATTICE) of LATTICE."],"metadata":{"id":"SMAuqWCeDkZu"}},{"cell_type":"code","source":["import gdown\n","import os\n","\n","gdown.download(f'https://drive.google.com/uc?id=1u_E3gJgjD-mMCE0GJ_P0slsVY-bO4Ph_', 'office.zip', quiet=False)\n","\n","!mkdir data\n","!mv office.zip data\n","%cd data\n","!unzip office.zip\n","!rm office.zip\n","%cd .."],"metadata":{"id":"aOhmGcEBDtsP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The downloaded multimodal dataset has the following structure:\n","\n","```\n","├── office\n","│   ├── image_feat\n","│       ├── 0.npy\n","│       ├── 1.npy\n","│       ├── ...\n","│   ├── text_feat\n","│       ├── 0.npy\n","│       ├── 1.npy\n","│       ├── ...\n","│   ├── train.txt\n","│   ├── validation.txt\n","│   ├── test.txt\n","```\n","\n"],"metadata":{"id":"csGP1_ZOuovk"}},{"cell_type":"markdown","source":["## Run experiments\n","Now we are all set to run an experiment with LightGCNM on Amazon Office."],"metadata":{"id":"OSiGJuqOb0Kq"}},{"cell_type":"code","source":["from elliot.run import run_experiment\n","\n","run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"KhT7EHQ6ZHvT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we overwrite the configuration file to train and test LATTICE on Amazon Office."],"metadata":{"id":"x_-3hMKog654"}},{"cell_type":"code","source":["config = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train.txt',\n","      'validation_path': '../data/{0}/val.txt',\n","      'test_path': '../data/{0}/test.txt',\n","      'side_information': [\n","        {\n","            'dataloader': 'VisualAttribute',\n","            'visual_features': '../data/{0}/image_feat'\n","        },\n","        {\n","            'dataloader': 'TextualAttribute',\n","            'textual_features': '../data/{0}/text_feat'\n","        }\n","      ]\n","    },\n","    'dataset': 'office',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'nDCG']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.LATTICE': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'save_recs': False,\n","          'validation_rate': 10,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'epochs': 200,\n","        'batch_size': 1024,\n","        'factors': 64,\n","        'lr': 0.005,\n","        'l_w': 1e-5,\n","        'n_layers': 1,\n","        'n_ui_layers': 2,\n","        'top_k': 20,\n","        'cf': 'lightgcn',\n","        'l_m': 0.7,\n","        'factors_multimod': 64,\n","        'modalities': \"('visual','textual')\",\n","        'loaders': \"('VisualAttribute','TextualAttribute')\",\n","        'seed': 123\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}', 'w') as file:\n","    documents = yaml.dump(config, file)\n","\n","run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"V2cJbr6BTIJ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Take a look at the code!\n","Each (multimodal-based) recommender system comes with three modules:\n","\n","\n","\n","```\n","├── model-name\n","│   ├── base-class\n","│   ├── model-class\n","│   ├── __init__.py\n","│   ├── <additional_class_1>\n","│   ├── <additional_class_2>\n","│   ├── ...\n","```\n","\n","For instance, let's consider LightGCNM:\n","\n","```\n","├── lightgcn_m\n","│   ├── LightGCNM.py\n","│   ├── LightGCNMModel.py\n","│   ├── __init__.py\n","│   ├── custom_sampler.py\n","```\n","\n","and LATTICE:\n","\n","```\n","├── lattice\n","│   ├── LATTICE.py\n","│   ├── LATTICEModel.py\n","│   ├── NGCFLayer.py\n","│   ├── __init__.py\n","│   ├── custom_sampler.py\n","```\n","\n","Moreover, recommender systems leveraging side-information (e.g., multimodal features) require 1+ dataset loader(s) to load and inject the additional features into the recommendation model.\n","\n","For instance, in the case of the visual, textual, and audio modalities, we have:\n","\n","```\n","├── visual\n","│   ├── __init__.py\n","│   ├── visual_attribute.py\n","```\n","\n","```\n","├── textual\n","│   ├── __init__.py\n","│   ├── textual_attribute.py\n","```\n","\n","```\n","├── audio\n","│   ├── __init__.py\n","│   ├── audio_attribute.py\n","```\n","\n","You can find them at ```elliot.dataset.modular_loaders```."],"metadata":{"id":"ud7Bxpt8maqz"}},{"cell_type":"markdown","source":["### Multimodal data loader\n","\n","Each modality loading and processing is handled by a specific data loader.\n","\n","For instance, for the visual modality, we have:\n","\n","```python\n","class VisualAttribute(AbstractLoader):\n","    def __init__(self, users: t.Set, items: t.Set, ns: SimpleNamespace, logger: object):\n","        \n","        # folder path for the visual features\n","        self.visual_feature_folder_path = getattr(ns, \"visual_features\", None)\n","\n","        # item mapping dictionary\n","        self.item_mapping = {}\n","\n","        # shape of visual features\n","        self.visual_features_shape = None\n","\n","        # get items having visual features\n","        inner_items = self.check_items_in_folder()\n","\n","        # align items having visual features and items from the training set\n","        self.items = items & inner_items\n","    \n","    # method to align items having visual features and items from the training set\n","    def check_items_in_folder(self) -> t.Set[int]:\n","       items = set()\n","        if self.visual_feature_folder_path:\n","\n","            # get the ids of all items having a visual feature\n","            items_folder = os.listdir(self.visual_feature_folder_path)\n","            items = items.union(set([int(f.split('.')[0]) for f in items_folder]))\n","            self.visual_features_shape = np.load(os.path.join(self.visual_feature_folder_path,\n","                                                              items_folder[0])).shape[0]\n","        if items:\n","            \n","            # map the ids of all items having a visual feature with our internal dictionary mapping\n","            self.item_mapping = {item: val for val, item in enumerate(items)}\n","        return items\n","\n","      # get all visual features as one numpy array\n","      def get_all_visual_features(self):\n","      all_features = np.empty((len(self.items), self.visual_features_shape))\n","      if self.visual_feature_folder_path:\n","          for key, value in self.item_mapping.items():\n","              all_features[value] = np.load(self.visual_feature_folder_path + '/' + str(key) + '.npy')\n","      return all_features\n","\n","      # this method is actually called from outside\n","      def get_all_features(self):\n","        return self.get_all_visual_features()\n","\n","      # method to create a namespace from this class\n","      def create_namespace(self) -> SimpleNamespace:\n","        ns = SimpleNamespace()\n","        ns.__name__ = \"VisualAttribute\"\n","        ns.object = self\n","        ns.visual_feature_folder_path = self.visual_feature_folder_path\n","\n","        ns.item_mapping = self.item_mapping\n","\n","        ns.visual_features_shape = self.visual_features_shape\n","        \n","        return ns\n","  \n","```\n","\n"],"metadata":{"id":"vc8MZGDjlhPM"}},{"cell_type":"markdown","source":["### Light Graph Convolutional Network with Multimodal Side Information (LightGCNM)\n","\n","Inspired by the baseline LightGCN-M, as presented in:\n","\n","Wei Wei, Chao Huang, Lianghao Xia, Chuxu Zhang: _Multi-Modal Self-Supervised Learning for Recommendation_. WWW 2023: 790-800\n","\n","\\[[**paper**](https://arxiv.org/abs/2302.10632)\\]"],"metadata":{"id":"Zn3OlPhhjLX_"}},{"cell_type":"markdown","source":["#### Base class\n","\n","Let's take a look at the base-class for LightGCNM (the file is ```LightGCNM.py```):\n","\n","```python\n","class LightGCNM(RecMixin, BaseRecommenderModel):\n","    r\"\"\"\n","    Light graph convolutional network with multimodal side information.\n","\n","    For further details, please refer to the `paper <https://dl.acm.org/doi/10.1145/3543507.3583206>`_\n","    \"\"\"\n","    @init_charger\n","    def __init__(self, data, config, params, *args, **kwargs):\n","\n","        # parameters list for LightGCNM\n","        self._params_list = [\n","            (\"_learning_rate\", \"lr\", \"lr\", 0.0005, float, None),\n","            (\"_factors\", \"factors\", \"factors\", 64, int, None),\n","            (\"_l_w\", \"l_w\", \"l_w\", 0.01, float, None),\n","            (\"_modalities\", \"modalities\", \"modalites\", \"('visual','textual')\", lambda x: list(make_tuple(x)),\n","             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n","            (\"_n_layers\", \"n_layers\", \"n_layers\", 1, int, None),\n","            (\"_normalize\", \"normalize\", \"normalize\", True, bool, None),\n","            (\"_aggregation\", \"aggregation\", \"aggregation\", 'mean', str, None),\n","            (\"_loaders\", \"loaders\", \"loads\", \"('VisualAttribute','TextualAttribute')\", lambda x: list(make_tuple(x)),\n","             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\"))\n","        ]\n","        self.autoset_params()\n","\n","        # create the sampler for BPR\n","        self._sampler = Sampler(self._data.i_train_dict, seed=self._seed)\n","\n","        # instantiate the adjacency matrix\n","        row, col = data.sp_i_train.nonzero()\n","        col = [c + self._num_users for c in col]\n","        edge_index = np.array([row, col])\n","        edge_index = torch.tensor(edge_index, dtype=torch.int64)\n","        self.adj = SparseTensor(row=torch.cat([edge_index[0], edge_index[1]], dim=0),\n","                                col=torch.cat([edge_index[1], edge_index[0]], dim=0),\n","                                sparse_sizes=(self._num_users + self._num_items,\n","                                              self._num_users + self._num_items))\n","\n","        # we associate one class attribute for each modality\n","        for m_id, m in enumerate(self._modalities):\n","            self.__setattr__(f'''_side_{m}''',\n","                             self._data.side_information.__getattribute__(f'''{self._loaders[m_id]}'''))\n","\n","        # load all multimodal features\n","        all_multimodal_features = []\n","        for m_id, m in enumerate(self._modalities):\n","            all_multimodal_features.append(self.__getattribute__(\n","                f'''_side_{self._modalities[m_id]}''').object.get_all_features())\n","\n","        # instantiate the model\n","        self._model = LightGCNMModel(\n","            num_users=self._num_users,\n","            num_items=self._num_items,\n","            learning_rate=self._learning_rate,\n","            embed_k=self._factors,\n","            l_w=self._l_w,\n","            n_layers=self._n_layers,\n","            adj=self.adj,\n","            modalities=self._modalities,\n","            multimodal_features=all_multimodal_features,\n","            aggregation=self._aggregation,\n","            normalize=self._normalize,\n","            random_seed=self._seed\n","        )\n","    ```\n"],"metadata":{"id":"9MNlT6OwthKH"}},{"cell_type":"markdown","source":["#### Model class\n","Let's take a look at the model-class for LightGCNM (the file is ```LightGCNMModel.py```):\n","\n","\n","```python\n","class LightGCNMModel(torch.nn.Module, ABC):\n","      def __init__(self,\n","                 num_users,\n","                 num_items,\n","                 learning_rate,\n","                 embed_k,\n","                 l_w,\n","                 n_layers,\n","                 adj,\n","                 modalities,\n","                 multimodal_features,\n","                 aggregation,\n","                 normalize,\n","                 random_seed,\n","                 name=\"LightGCNM\",\n","                 **kwargs\n","                 ):\n","        super().__init__()\n","\n","        # set all seeds and deterministic behaviour\n","        random.seed(random_seed)\n","        np.random.seed(random_seed)\n","        torch.manual_seed(random_seed)\n","        torch.cuda.manual_seed(random_seed)\n","        torch.cuda.manual_seed_all(random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.use_deterministic_algorithms(True)\n","\n","        # set device\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # set all model's parameters\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.embed_k = embed_k\n","        self.learning_rate = learning_rate\n","        self.l_w = l_w\n","        self.n_layers = n_layers\n","        self.weight_size_list = [self.embed_k] * (self.n_layers + 1)\n","        self.alpha = torch.tensor([1 / (k + 1) for k in range(len(self.weight_size_list))])\n","        self.adj = adj\n","        self.normalize = normalize\n","\n","        # create node embeddings\n","        self.Gu = torch.nn.Embedding(\n","            num_embeddings=self.num_users, embedding_dim=self.embed_k)\n","        torch.nn.init.xavier_uniform_(self.Gu.weight)\n","\n","        # multimodal attributes\n","        self.modalities = modalities\n","        self.aggregation = aggregation\n","\n","        # multimodal features and projection network\n","        self.F = torch.nn.ParameterList()\n","        if self.aggregation == 'concat':\n","            total_multimodal_features = 0\n","            for m_id, m in enumerate(self.modalities):\n","                self.F.append(torch.nn.Embedding.from_pretrained(torch.tensor(\n","                    multimodal_features[m_id], device=self.device, dtype=torch.float32),\n","                    freeze=False))\n","                total_multimodal_features += multimodal_features[m_id].shape[-1]\n","            self.proj = torch.nn.Linear(in_features=total_multimodal_features, out_features=self.embed_k)\n","        else:\n","            self.proj = torch.nn.ModuleList()\n","            for m_id, m in enumerate(self.modalities):\n","                self.F.append(torch.nn.Embedding.from_pretrained(torch.tensor(\n","                    multimodal_features[m_id], device=self.device, dtype=torch.float32),\n","                    freeze=False))\n","                self.proj.append(\n","                    torch.nn.Linear(in_features=multimodal_features[m_id].shape[-1], out_features=self.embed_k))\n","        self.F.to(self.device)\n","        self.proj.to(self.device)\n","\n","        # create GNN (the same as LightGCN)\n","        propagation_network_list = []\n","\n","        for _ in range(self.n_layers):\n","            propagation_network_list.append((LGConv(normalize=self.normalize), 'x, edge_index -> x'))\n","\n","        self.propagation_network = torch_geometric.nn.Sequential('x, edge_index', propagation_network_list)\n","        self.propagation_network.to(self.device)\n","\n","        # instantiate optimizer\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","    \n","    # method to run message-passing\n","    def propagate_embeddings(self, adj):\n","\n","        # project multimodal feautures in the same latent space as the user embeddings, and then fuse them\n","        if self.aggregation == 'concat':\n","            F_proj = torch.nn.functional.normalize(\n","                self.proj(torch.concat([self.F[m_id].weight\n","                                        for m_id in range(len(self.F))], dim=-1).to(self.device)\n","                          ), p=2, dim=1).to(self.device)\n","        elif self.aggregation == 'mean':\n","            F_proj = [torch.nn.functional.normalize(self.proj[m_id](self.F[m_id].weight).to(self.device), p=2, dim=1)\n","                      for m_id in range(len(self.F))]\n","            F_proj = torch.mean(torch.stack(F_proj, dim=-1).to(self.device), dim=-1)\n","        elif self.aggregation == 'sum':\n","            F_proj = [torch.nn.functional.normalize(self.proj[m_id](self.F[m_id].weight).to(self.device), p=2, dim=1)\n","                      for m_id in range(len(self.F))]\n","            F_proj = torch.sum(torch.stack(F_proj, dim=-1).to(self.device), dim=-1)\n","\n","        # then, run the same message-passing as LightGCN, but item nodes features are multimodal\n","        ego_embeddings = torch.cat((self.Gu.weight.to(self.device), F_proj.to(self.device)), 0)\n","        all_embeddings = [ego_embeddings]\n","\n","        for layer in range(0, self.n_layers):\n","            if evaluate:\n","                self.propagation_network.eval()\n","                with torch.no_grad():\n","                    all_embeddings += [list(\n","                        self.propagation_network.children()\n","                    )[layer](all_embeddings[layer].to(self.device), self.adj.to(self.device))]\n","            else:\n","                all_embeddings += [list(\n","                    self.propagation_network.children()\n","                )[layer](all_embeddings[layer].to(self.device), self.adj.to(self.device))]\n","\n","        if evaluate:\n","            self.propagation_network.train()\n","\n","        all_embeddings = torch.mean(torch.stack(all_embeddings, 0), dim=0)\n","        gu, fi = torch.split(all_embeddings, [self.num_users, self.num_items], 0)\n","        return gu, fi\n","```\n","\n"],"metadata":{"id":"ZHp0BoMBtqTw"}},{"cell_type":"markdown","source":["### LATent sTructure mining method for multImodal reCommEndation (LATTICE)\n","\n","Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, Liang Wang: _Mining Latent Structures for Multimedia Recommendation_. ACM Multimedia 2021: 3872-3880\n","\n","<div>\n","  <img src=\"https://dl.acm.org/cms/asset/6d22a6ae-6b80-431c-9092-ef5e8a52736c/3474085.3475259.key.jpg\" alt=\"LATTICE\" width=\"700\">\n","</div>\n","\n","\\[[**paper**](https://arxiv.org/abs/2104.09036)\\]\\[[**code**](https://github.com/CRIPAC-DIG/LATTICE)\\]"],"metadata":{"id":"2pqte9dekXHh"}},{"cell_type":"markdown","source":["#### Base class\n","\n","Let's take a look at the base-class for LATTICE (the file is ```LATTICE.py```):\n","\n","```python\n","class LATTICE(RecMixin, BaseRecommenderModel):\n","    r\"\"\"\n","    Mining Latent Structures for Multimedia Recommendation\n","\n","    For further details, please refer to the `paper <https://dl.acm.org/doi/10.1145/3474085.3475259>`_\n","    \"\"\"\n","    @init_charger\n","    def __init__(self, data, config, params, *args, **kwargs):\n","\n","        # parameters list for LATTICE\n","        self._params_list = [\n","            (\"_learning_rate\", \"lr\", \"lr\", 0.0005, float, None),\n","            (\"_factors\", \"factors\", \"factors\", 64, int, None),\n","            (\"_l_w\", \"l_w\", \"l_w\", 0.01, float, None),\n","            (\"_n_layers\", \"n_layers\", \"n_layers\", 1, int, None),\n","            (\"_n_ui_layers\", \"n_ui_layers\", \"n_ui_layers\", 3, int, None),\n","            (\"_top_k\", \"top_k\", \"top_k\", 100, int, None),\n","            (\"_factors_multimod\", \"factors_multimod\", \"factors_multimod\", 64, int, None),\n","            (\"_cf\", \"cf\", \"cf\", 'lightgcn', str, None),\n","            (\"_modalities\", \"modalities\", \"modalites\", \"('visual','textual')\", lambda x: list(make_tuple(x)),\n","             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n","            (\"_lambda\", \"l_m\", \"l_m\", 0.1, float, None),\n","            (\"_ws\", \"ws\", \"ws\", \"(64,64,64)\", lambda x: list(make_tuple(x)),\n","             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n","            (\"_dl\", \"dl\", \"dl\", \"(0.1,0.1,0.1)\", lambda x: list(make_tuple(x)),\n","             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\")),\n","            (\"_loaders\", \"loaders\", \"loads\", \"('VisualAttribute','TextualAttribute')\", lambda x: list(make_tuple(x)),\n","             lambda x: self._batch_remove(str(x), \" []\").replace(\",\", \"-\"))\n","        ]\n","        self.autoset_params()\n","\n","        # create the sampler for BPR\n","        self._sampler = Sampler(self._data.i_train_dict, self._batch_size, self._seed)\n","\n","        # instantiate the adjacency matrix\n","        row, col = data.sp_i_train.nonzero()\n","        col = [c + self._num_users for c in col]\n","        edge_index = np.array([row, col])\n","        edge_index = torch.tensor(edge_index, dtype=torch.int64)\n","        self.adj = SparseTensor(row=torch.cat([edge_index[0], edge_index[1]], dim=0),\n","                                col=torch.cat([edge_index[1], edge_index[0]], dim=0),\n","                                sparse_sizes=(self._num_users + self._num_items,\n","                                              self._num_users + self._num_items))\n","        \n","        # we associate one class attribute for each modality\n","        for m_id, m in enumerate(self._modalities):\n","            self.__setattr__(f'''_side_{m}''',\n","                             self._data.side_information.__getattribute__(f'''{self._loaders[m_id]}'''))\n","\n","        # load all multimodal features\n","        all_multimodal_features = []\n","        for m_id, m in enumerate(self._modalities):\n","            all_multimodal_features.append(self.__getattribute__(\n","                f'''_side_{self._modalities[m_id]}''').object.get_all_features())\n","\n","\n","        # instantiate the model\n","        self._model = LATTICEModel(\n","            num_users=self._num_users,\n","            num_items=self._num_items,\n","            num_layers=self._n_layers,\n","            num_ui_layers=self._n_ui_layers,\n","            learning_rate=self._learning_rate,\n","            embed_k=self._factors,\n","            embed_k_multimod=self._factors_multimod,\n","            l_w=self._l_w,\n","            modalities=self._modalities,\n","            l_m=self._lambda,\n","            top_k=self._top_k,\n","            multimodal_features=all_multimodal_features,\n","            adj=self.adj,\n","            cf_model=self._cf,\n","            weight_size=self._ws,\n","            dropout_list=self._dl,\n","            random_seed=self._seed\n","        )\n","```\n"],"metadata":{"id":"tT76sAYJmrVT"}},{"cell_type":"markdown","source":["#### Model class\n","\n","---\n","\n","**Disclaimer #1.** The current formulas may not be completely aligned with the original paper because they are obtained from the algorithm as explained in the original paper and **code**.\n","\n","---\n","\n","**Disclaimer #2.** The following operations are performed batch-wise. They require an initialization where $\\hat{\\mathbf{S}}^m_{\\text{curr}}$ is calculated through the equations (1), (2), (3), and a final normalization.\n","\n","---\n","\n","First, we project the items' multimodal features into another latent space:\n","\n","$$\\tilde{\\mathbf{e}}_i^{m} = \\mathbf{W}_m\\mathbf{e}_i^{m} + \\mathbf{b}_m. \\quad (1)$$\n","\n","Second, we calculate the similarity matrix for the item-item projected graph, and apply kNN-sparsification:\n","\n","$$\\mathbf{S}_{ij}^{m} = \\frac{(\\tilde{\\mathbf{e}}_i^{m})^\\top\\tilde{\\mathbf{e}}_j^{m}}{||\\tilde{\\mathbf{e}}_i^m||\\;||\\tilde{\\mathbf{e}}_j^m||}, \\quad (2)$$\n","$$\\tilde{\\mathbf{S}}_{ij}^{m} = \\begin{cases}\n","\\mathbf{S}_{ij}^m, \\quad \\mathbf{S}_{ij}^m \\in \\text{top-}k(\\mathbf{S}_i^m)\\\\\n","0 \\quad \\text{otherwise}.\n","\\end{cases} \\quad (3)$$\n","\n","Third, we compute the weighted sum of all similarity matrices for each modality. Please note that, according to the original code, the weights are normalized through the softmax:\n","\n","$$\\mathbf{A}_{\\text{learn}} = \\underbrace{\\sum_{m=0}^{|\\mathcal{M}|}\\underbrace{\\alpha_m\\tilde{\\mathbf{S}}^{m}}_{(4.1a)}}_{(4.2a)}. \\quad (4a)$$\n","$$\\mathbf{A}_{\\text{curr}} = \\underbrace{\\sum_{m=0}^{|\\mathcal{M}|}\\underbrace{\\alpha_m\\hat{\\mathbf{S}}_{\\text{curr}}^{m}}_{(4.1b)}}_{(4.2b)}. \\quad (4b)$$\n","\n","Then, the obtained adjacency matrix is normalized:\n","\n","$$\\hat{\\mathbf{A}}_{\\text{learn}} = (\\mathbf{D})^{-1/2}\\tilde{\\mathbf{A}}_{\\text{learn}}(\\mathbf{D})^{-1/2}, \\quad (5)$$\n","\n","and we update the adjacency matrix with the previous one (from the previous iteration):\n","\n","$$\\mathbf{A} = \\lambda\\mathbf{A}_{\\text{curr}} + (1 - \\lambda)\\hat{\\mathbf{A}}_{\\text{learn}}. \\quad (6)$$\n","\n","Finally, we obtain the final item embeddings by propagating the messages on the updated item-item similarity matrix:\n","\n","$$\\mathbf{h}_i^{(l)} = \\sum_{j \\in \\mathcal{N}_i}\\mathbf{A}_{ij}\\mathbf{h}_j^{(l-1)}, \\quad (7)$$\n","\n","where $\\mathbf{h}$ is the item collaborative embedding.\n","\n","Let's take a look at the model-class for LATTICE (the file is ```LATTICEModel.py```):\n","\n","\n","```python\n","class LATTICEModel(torch.nn.Module, ABC):\n","      def __init__(self,\n","                 num_users,\n","                 num_items,\n","                 num_layers,\n","                 num_ui_layers,\n","                 learning_rate,\n","                 embed_k,\n","                 embed_k_multimod,\n","                 l_w,\n","                 modalities,\n","                 l_m,\n","                 top_k,\n","                 multimodal_features,\n","                 adj,\n","                 cf_model,\n","                 weight_size,\n","                 dropout_list,\n","                 random_seed,\n","                 name=\"LATTICE\",\n","                 **kwargs\n","                 ):\n","        super().__init__()\n","\n","        # set all seeds and deterministic behaviour\n","        random.seed(random_seed)\n","        np.random.seed(random_seed)\n","        torch.manual_seed(random_seed)\n","        torch.cuda.manual_seed(random_seed)\n","        torch.cuda.manual_seed_all(random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.use_deterministic_algorithms(True)\n","\n","        # set device\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # set all model's parameters\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.embed_k = embed_k\n","        self.learning_rate = learning_rate\n","        self.l_w = l_w\n","        self.n_ui_layers = num_ui_layers\n","        self.cf_model = cf_model\n","        self.adj = adj\n","        self.weight_size = weight_size\n","        self.weight_size = [self.embed_k] + self.weight_size\n","\n","        # multimodal attributes\n","        self.embed_k_multimod = embed_k_multimod\n","        self.modalities = modalities\n","        self.l_m = l_m\n","        self.n_layers = num_layers\n","        self.top_k = top_k\n","\n","        # create collaborative node embeddings\n","        self.Gu = torch.nn.Embedding(self.num_users, self.embed_k)\n","        self.Gi = torch.nn.Embedding(self.num_items, self.embed_k)\n","        torch.nn.init.xavier_uniform_(self.Gu.weight)\n","        torch.nn.init.xavier_uniform_(self.Gi.weight)\n","        self.Gu.to(self.device)\n","        self.Gi.to(self.device)\n","\n","        # multimodal features and networks\n","        self.Gim = torch.nn.ParameterDict()\n","        self.Sim = dict()\n","        self.Si = None\n","        self.projection_m = torch.nn.ModuleDict()\n","        self.importance_weights_m = torch.nn.Parameter(\n","            torch.tensor(len(self.modalities) * [float(1 / len(self.modalities))]))\n","        self.importance_weights_m.to(self.device)\n","        self.multimodal_features_shapes = [mf.shape[1] for mf in multimodal_features]\n","        ir = torch.tensor(list(range(self.num_items)), dtype=torch.int64, device=self.device)\n","        self.items_rows = torch.repeat_interleave(ir, self.top_k).to(self.device)\n","        for m_id, m in enumerate(modalities):\n","            self.Gim[m] = torch.nn.Embedding.from_pretrained(\n","                torch.tensor(multimodal_features[m_id], dtype=torch.float32, device=self.device),\n","                freeze=False).weight\n","            self.Gim[m].to(self.device)\n","            current_sim = self.build_sim(self.Gim[m].detach())\n","            weighted_adj = self.build_knn_neighbourhood(current_sim, self.top_k)\n","            self.Sim[m] = self.compute_normalized_laplacian(weighted_adj, 0.5)\n","            self.Sim[m].to(self.device)\n","            self.projection_m[m] = torch.nn.Linear(in_features=self.multimodal_features_shapes[m_id],\n","                                                   out_features=self.embed_k_multimod)\n","            self.projection_m[m].to(self.device)\n","\n","        # item-item multimodal GNN\n","        propagation_network_list = []\n","        for layer in range(self.n_layers):\n","            propagation_network_list.append((LGConv(normalize=False), 'x, edge_index -> x'))\n","        self.propagation_network = torch_geometric.nn.Sequential('x, edge_index', propagation_network_list)\n","        self.propagation_network.to(self.device)\n","\n","        # backbone\n","        if cf_model == 'ngcf':\n","            propagation_network_list = []\n","            self.dropout_layers = []\n","            for layer in range(self.n_ui_layers):\n","                propagation_network_list.append((NGCFLayer(self.weight_size[layer],\n","                                                           self.weight_size[layer + 1]), 'x, edge_index -> x'))\n","                self.dropout_layers.append(torch.nn.Dropout(p=dropout_list[layer]))\n","            self.propagation_network_recommend = torch_geometric.nn.Sequential('x, edge_index', propagation_network_list)\n","            self.propagation_network_recommend.to(self.device)\n","\n","        elif cf_model == 'lightgcn':\n","            propagation_network_list = []\n","            for layer in range(self.n_ui_layers):\n","                propagation_network_list.append((LGConv(normalize=False), 'x, edge_index -> x'))\n","            self.propagation_network_recommend = torch_geometric.nn.Sequential('x, edge_index',\n","                                                                               propagation_network_list)\n","            self.propagation_network_recommend.to(self.device)\n","\n","        # instantiate optimizer\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","        self.lr_scheduler = self.set_lr_scheduler()\n","\n","    # method to build the item-item multimodal similarity matrix\n","    @staticmethod\n","    def build_sim(context):\n","        context_norm = context.div(torch.norm(context, p=2, dim=-1, keepdim=True))\n","        sim = torch.mm(context_norm, context_norm.transpose(1, 0))\n","        return sim\n","\n","    # method to perform knn-sparsification\n","    def build_knn_neighbourhood(self, adj, topk):\n","        knn_val, knn_ind = torch.topk(adj, topk, dim=-1)\n","        items_cols = torch.flatten(knn_ind).to(self.device)\n","        values = torch.flatten(knn_val).to(self.device)\n","        weighted_adj = SparseTensor(row=self.items_rows,\n","                                    col=items_cols,\n","                                    value=values,\n","                                    sparse_sizes=(self.num_items, self.num_items))\n","        return weighted_adj\n","\n","    # method to build the lr_scheduler\n","    def set_lr_scheduler(self):\n","      scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lambda epoch: 0.96 ** (epoch / 50))\n","      return scheduler\n","\n","    # train step batch-by-batch\n","    def train_step(self, batch, adj):\n","\n","        # run message-passing on the whole GNN\n","        gum, gim = self.propagate_embeddings(build_item_graph)\n","\n","        # run MF\n","        user, pos, neg = batch\n","        xu_pos, gamma_u_m, gamma_i_pos_m = self.forward(inputs=(gum[user], gim[pos]))\n","        xu_neg, _, gamma_i_neg_m = self.forward(inputs=(gum[user], gim[neg]))\n","\n","        # compute loss\n","        loss = -torch.mean(torch.nn.functional.logsigmoid(xu_pos - xu_neg))\n","        reg_loss = self.l_w * (1 / 2) * ((gamma_u_m**2).sum() + (gamma_i_pos_m**2).sum() + (gamma_i_neg_m**2).sum()) / len(user)\n","        loss += reg_loss\n","\n","        # backward propagation\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # return batch loss\n","        return loss.detach().cpu().numpy()\n","    \n","    # method to run message-passing\n","    def propagate_embeddings(self, build_item_graph=False):\n","\n","        if build_item_graph:\n","            weights = torch.cat([torch.unsqueeze(w, 0) for w in self.importance_weights_m], dim=0)\n","            softmax_weights = self.softmax(weights)\n","            learned_adj_addendum = []\n","            original_adj_addendum = []\n","            for m_id, m in enumerate(self.modalities):\n","                \n","                # eq. (1)\n","                projected_m = self.projection_m[m](self.Gim[m].to(self.device))\n","\n","                # eq. (2)\n","                current_sim = self.build_sim(projected_m)\n","\n","                # eq. (3)\n","                weighted_adj = self.build_knn_neighbourhood(current_sim, self.top_k)\n","\n","                # eq. (4.1a)\n","                learned_adj_addendum.append(mul_nnz(weighted_adj,\n","                                                    softmax_weights[m_id].repeat((weighted_adj.nnz(),)).to(\n","                                                        self.device),\n","                                                    layout='coo'))\n","                \n","                # eq. (4.1b)\n","                original_adj_addendum.append(mul_nnz(self.Sim[m],\n","                                                     softmax_weights[m_id].repeat((self.Sim[m].nnz(),)).to(\n","                                                         self.device),\n","                                                     layout='coo'))\n","            learned_adj = learned_adj_addendum[0]\n","\n","            # eq. (4.2a)\n","            for i in range(1, len(learned_adj_addendum)):\n","                learned_adj = add(learned_adj, learned_adj_addendum[i])\n","\n","            # eq. (5) from the original paper\n","            learned_adj = self.compute_normalized_laplacian(learned_adj, 0.5)\n","            original_adj = original_adj_addendum[0]\n","\n","            # eq. (4.2b) from the original paper\n","            for i in range(1, len(original_adj_addendum)):\n","                original_adj = add(original_adj, original_adj_addendum[i])\n","\n","            # eq. (6)\n","            first = mul_nnz(learned_adj, torch.tensor([1 - self.l_m]).repeat((learned_adj.nnz(),)).to(self.device),\n","                            layout='coo')\n","            second = mul_nnz(original_adj, torch.tensor([self.l_m]).repeat((original_adj.nnz(),)).to(self.device),\n","                             layout='coo')\n","            self.Si = add(first, second)\n","        else:\n","            self.Si = self.Si.detach()\n","\n","        # eq. (7)\n","        item_embedding = self.Gi.weight\n","        for layer in range(self.n_layers):\n","            item_embedding = list(self.propagation_network.children())[layer](item_embedding.to(self.device),\n","                                                                              self.Si)\n","\n","        ego_embeddings = torch.cat((self.Gu.weight.to(self.device), self.Gi.weight.to(self.device)), 0)\n","        all_embeddings = [ego_embeddings]\n","\n","        if self.cf_model == 'ngcf':\n","            embedding_idx = 0\n","            for layer in range(self.n_ui_layers):\n","                all_embeddings += [torch.nn.functional.normalize(self.dropout_layers[embedding_idx](list(\n","                    self.propagation_network_recommend.children()\n","                )[layer](all_embeddings[embedding_idx].to(self.device), self.adj.to(self.device))), p=2, dim=1)]\n","                embedding_idx += 1\n","            all_embeddings = torch.stack(all_embeddings, dim=1)\n","            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)\n","            gu, gi = torch.split(all_embeddings, [self.num_users, self.num_items], 0)\n","            return gu, gi + torch.nn.functional.normalize(item_embedding.to(self.device), p=2, dim=1)\n","\n","        elif self.cf_model == 'lightgcn':\n","            for layer in range(self.n_ui_layers):\n","                all_embeddings += [torch.nn.functional.normalize(list(\n","                    self.propagation_network_recommend.children()\n","                )[layer](all_embeddings[layer].to(self.device), self.adj.to(self.device)), p=2, dim=1)]\n","            all_embeddings = torch.stack(all_embeddings, dim=1)\n","            all_embeddings = all_embeddings.mean(dim=1, keepdim=False)\n","            gu, gi = torch.split(all_embeddings, [self.num_users, self.num_items], 0)\n","            return gu, gi + torch.nn.functional.normalize(item_embedding.to(self.device), p=2, dim=1)\n","\n","        elif self.cf_model == 'mf':\n","            return self.Gu.weight, self.Gi.weight + torch.nn.functional.normalize(item_embedding.to(self.device), p=2, dim=1)\n","\n","    # method to run MF\n","    def forward(self, inputs, **kwargs):\n","        gum, gim = inputs\n","        gamma_u_m = torch.squeeze(gum).to(self.device)\n","        gamma_i_m = torch.squeeze(gim).to(self.device)\n","\n","        xui = torch.sum(gamma_u_m * gamma_i_m, 1)\n","\n","        return xui, gamma_u_m, gamma_i_m\n","```\n","\n"],"metadata":{"id":"1AL4zI9gnS4H"}},{"cell_type":"markdown","source":["## Cite us\n","\n","This notebook is heavily dependent on [Elliot](https://github.com/sisinflab/elliot), our framework for rigorous and reproducible recommender systems evaluation. If you use this in your works, please consider to cite us:\n","\n","\n","\n","```\n","@inproceedings{DBLP:conf/sigir/AnelliBFMMPDN21,\n","  author       = {Vito Walter Anelli and\n","                  Alejandro Bellog{\\'{\\i}}n and\n","                  Antonio Ferrara and\n","                  Daniele Malitesta and\n","                  Felice Antonio Merra and\n","                  Claudio Pomo and\n","                  Francesco Maria Donini and\n","                  Tommaso Di Noia},\n","  title        = {Elliot: {A} Comprehensive and Rigorous Framework for Reproducible\n","                  Recommender Systems Evaluation},\n","  booktitle    = {{SIGIR}},\n","  pages        = {2405--2414},\n","  publisher    = {{ACM}},\n","  year         = {2021}\n","}\n","```\n","\n","\n","\n","```\n","@article{DBLP:journals/corr/abs-2309-05273,\n","  author       = {Daniele Malitesta and\n","                  Giandomenico Cornacchia and\n","                  Claudio Pomo and\n","                  Felice Antonio Merra and\n","                  Tommaso Di Noia and\n","                  Eugenio Di Sciascio},\n","  title        = {Formalizing Multimedia Recommendation through Multimodal Deep Learning},\n","  journal      = {CoRR},\n","  volume       = {abs/2309.05273},\n","  year         = {2023}\n","}\n","```\n","\n","\n","```\n","@inproceedings{10.1145/3606040.3617441,\n","  author       = {Malitesta, Daniele and Cornacchia, Giandomenico and Pomo, Claudio and Di Noia, Tommaso},\n","  title        = {On Popularity Bias of Multimodal-Aware Recommender Systems: A Modalities-Driven Analysis},\n","  year        = {2023},\n","  isbn        = {9798400702716},\n","  publisher   = {Association for Computing Machinery},\n","  address     = {New York, NY, USA},\n","  url         = {https://doi.org/10.1145/3606040.3617441},\n","  doi         = {10.1145/3606040.3617441},\n","  booktitle   = {Proceedings of the 1st International Workshop on Deep Multimodal Learning for Information Retrieval},\n","  pages       = {59–68},\n","  numpages    = {10},\n","  series = {MMIR '23}\n","}\n","```\n","\n","\n","```\n","@inproceedings{DBLP:conf/kdd/MalitestaCPN23,\n","  author       = {Daniele Malitesta and\n","                  Giandomenico Cornacchia and\n","                  Claudio Pomo and\n","                  Tommaso Di Noia},\n","  title        = {Disentangling the Performance Puzzle of Multimodal-aware Recommender\n","                  Systems},\n","  booktitle    = {EvalRS@KDD},\n","  series       = {{CEUR} Workshop Proceedings},\n","  volume       = {3450},\n","  publisher    = {CEUR-WS.org},\n","  year         = {2023}\n","}\n","```"],"metadata":{"id":"TSm0PwiIkecu"}}]}