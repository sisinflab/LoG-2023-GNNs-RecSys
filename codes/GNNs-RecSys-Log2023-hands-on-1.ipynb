{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyPbA3chQKgffGC48XxsHYei"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# [Hands-on #1] Reproducibility\n","\n","\n","<div>\n","  <img src=\"https://logconference.org/post/announcement/featured.png\" alt=\"Poliba\" width=\"100\">\n","</div>\n","\n","This is the Google Colab notebook for the hands-on session #1 of the tutorial: \"_[Graph Neural Networks for Recommendation: Reproducibility, Graph Topology, and Node Representation](https://sisinflab.github.io/tutorial-gnns-recsys-log2023/)_\" presented at the [2nd Learning on Graphs Conference](https://logconference.org/) (LoG 2023) -- November, 30 (Online).\n","\n","Credits:\n","- Daniele Malitesta (daniele.malitesta@poliba.it)\n","- Claudio Pomo (claudio.pomo@poliba.it)\n","- Tommaso Di Noia (tommaso.dinoia@poliba.it)\n","\n","<div>\n","  <img src=\"https://www.poliba.it/sites/default/files/logo_5.png\" alt=\"Poliba\" width=\"200\">\n","  <img src=\"https://swot.sisinflab.poliba.it/img/logo-sisinflab.png\" alt=\"SisInfLab\" width=\"200\">\n","</div>"],"metadata":{"id":"3KiLoN1pcxP4"}},{"cell_type":"markdown","source":["## Clone the repository\n","\n","First, let's clone the repository from GitHub..."],"metadata":{"id":"eluRHZzVR_Au"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXFH1E8xL-KC"},"outputs":[],"source":["!git clone https://github.com/sisinflab/Graph-RSs-Reproducibility.git"]},{"cell_type":"markdown","source":["## Set up the environment\n","\n","Second, let's set up the environment with the needed (extra) pip packages and the environment variables to ensure reproducibility!"],"metadata":{"id":"mHW69OKpSGoK"}},{"cell_type":"code","source":["%cd Graph-RSs-Reproducibility/\n","%env PYTHONPATH=.\n","%env CUBLAS_WORKSPACE_CONFIG=:16:8\n","!pip install -r requirements_torch_geometric_colab.txt"],"metadata":{"id":"qubulQo1K6SP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check if GPU is available\n","\n","Then, let's check if the GPU is available:"],"metadata":{"id":"Sk8_rR50VFe6"}},{"cell_type":"code","source":["!nvidia-smi\n","!nvcc --version"],"metadata":{"id":"cq-j--VVVQ3U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Configure the experiments\n","Let's set the hyper-parameters for the model to be trained and tested. We begin with NGCF on Gowalla."],"metadata":{"id":"HqcVn3KaRUkg"}},{"cell_type":"code","source":["import yaml\n","\n","config_filename = 'hands-on-1-log_2023.yml'\n","config = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train.tsv',\n","      'test_path': '../data/{0}/test.tsv'\n","    },\n","    'dataset': 'gowalla',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'nDCG']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.NGCF': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'validation_rate': 10,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'lr': 0.0001,\n","        'epochs': 400,\n","        'factors': 64,\n","        'batch_size': 1024,\n","        'l_w': 1e-5,\n","        'n_layers': 3,\n","        'weight_size': 64,\n","        'node_dropout': 0.1,\n","        'message_dropout': 0.1,\n","        'normalize': True,\n","        'seed': 42,\n","        'early_stopping': {\n","          'patience': 5,\n","          'mode': 'auto',\n","          'monitor': 'Recall@20',\n","          'verbose': True\n","        }\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}', 'w') as file:\n","    documents = yaml.dump(config, file)"],"metadata":{"id":"V2cJbr6BTIJ9","executionInfo":{"status":"ok","timestamp":1701168117366,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniele Malitesta","userId":"17706482384632116882"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Run experiments\n","Now we are all set to run an experiment with NGCF on Gowalla."],"metadata":{"id":"OSiGJuqOb0Kq"}},{"cell_type":"code","source":["from elliot.run import run_experiment\n","\n","run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"KhT7EHQ6ZHvT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we overwrite the configuration file to train and test LightGCN on Gowalla."],"metadata":{"id":"iQ3h19aRr1Om"}},{"cell_type":"code","source":["import yaml\n","\n","config_filename = 'hands-on-1-log_2023.yml'\n","config = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train.tsv',\n","      'test_path': '../data/{0}/test.tsv'\n","    },\n","    'dataset': 'gowalla',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'nDCG']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.LightGCN': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'validation_rate': 20,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'lr': 0.001,\n","        'epochs': 1000,\n","        'factors': 64,\n","        'batch_size': 2048,\n","        'l_w': 1e-4,\n","        'n_layers': 3,\n","        'normalize': True,\n","        'seed': 42,\n","        'early_stopping': {\n","          'patience': 5,\n","          'mode': 'auto',\n","          'monitor': 'Recall@20',\n","          'verbose': True\n","        }\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}', 'w') as file:\n","    documents = yaml.dump(config, file)\n","\n","run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"YsTG-hiVrqH1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Take a look at the code!\n","Each (GNNs-based) recommender system comes with three modules:\n","\n","\n","\n","```\n","├── model-name\n","│   ├── base-class\n","│   ├── model-class\n","│   ├── __init__.py\n","│   ├── <additional_class_1>\n","│   ├── <additional_class_2>\n","│   ├── ...\n","```\n","\n","For instance, let's consider NGCF:\n","\n","```\n","├── ngcf\n","│   ├── NGCF.py\n","│   ├── NGCFModel.py\n","│   ├── NGCFLayer.py\n","│   ├── __init__.py\n","│   ├── custom_sampler.py\n","```\n","\n","and LightGCN:\n","\n","```\n","├── lightgcn\n","│   ├── LightGCN.py\n","│   ├── LightGCNModel.py\n","│   ├── __init__.py\n","│   ├── custom_sampler.py\n","```"],"metadata":{"id":"ud7Bxpt8maqz"}},{"cell_type":"markdown","source":["### Neural Graph Collaborative Filtering (NGCF)\n","\n","Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua: _Neural Graph Collaborative Filtering_. SIGIR 2019: 165-174\n","\n","<div>\n","  <img src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6cBxhbNs9acjejFvsiqXMw.png\" alt=\"NGCF\" width=\"600\">\n","</div>\n","\n","\\[[**paper**](https://arxiv.org/abs/1905.08108)\\]\\[[**code**](https://github.com/huangtinglin/NGCF-PyTorch)\\]"],"metadata":{"id":"Zn3OlPhhjLX_"}},{"cell_type":"markdown","source":["#### Base class\n","\n","Let's take a look at the base-class for NGCF (the file is ```NGCF.py```):\n","\n","```python\n","class NGCF(RecMixin, BaseRecommenderModel):\n","    r\"\"\"\n","    Neural Graph Collaborative Filtering\n","\n","    For further details, please refer to the `paper <https://dl.acm.org/doi/10.1145/3331184.3331267>`_\n","    \"\"\"\n","    @init_charger\n","    def __init__(self, data, config, params, *args, **kwargs):\n","\n","        # parameters list for NGCF\n","        self._params_list = [\n","            (\"_learning_rate\", \"lr\", \"lr\", 0.0005, float, None),\n","            (\"_factors\", \"factors\", \"factors\", 64, int, None),\n","            (\"_l_w\", \"l_w\", \"l_w\", 0.01, float, None),\n","            (\"_n_layers\", \"n_layers\", \"n_layers\", 3, int, None),\n","            (\"_weight_size\", \"weight_size\", \"weight_size\", 64, int, None),\n","            (\"_node_dropout\", \"node_dropout\", \"node_dropout\", 0.0, float, None),\n","            (\"_message_dropout\", \"message_dropout\", \"message_dropout\", 0.5, float, None),\n","            (\"_normalize\", \"normalize\", \"normalize\", True, bool, None)\n","        ]\n","        self.autoset_params()\n","\n","        # set all seeds for reproducibility\n","        random.seed(self._seed)\n","        np.random.seed(self._seed)\n","        torch.manual_seed(self._seed)\n","\n","        # create the sampler for BPR\n","        self._sampler = Sampler(self._data.i_train_dict, self._batch_size, self._seed)\n","\n","        # instantiate the adjacency matrix\n","        row, col = data.sp_i_train.nonzero()\n","        col = [c + self._num_users for c in col]\n","        self.edge_index = np.array([row, col])\n","        self.adj = SparseTensor(row=torch.cat([torch.tensor(self.edge_index[0], dtype=torch.int64),\n","                                               torch.tensor(self.edge_index[1], dtype=torch.int64)], dim=0),\n","                                col=torch.cat([torch.tensor(self.edge_index[1], dtype=torch.int64),\n","                                               torch.tensor(self.edge_index[0], dtype=torch.int64)], dim=0),\n","                                sparse_sizes=(self._num_users + self._num_items,\n","                                              self._num_users + self._num_items))\n","\n","        # optionally normalize the adjacency matrix\n","        if self._normalize:\n","            self.adj = self.apply_norm(self.adj, add_self_loops=True)\n","\n","        # instantiate the model\n","        self._model = NGCFModel(\n","            num_users=self._num_users,\n","            num_items=self._num_items,\n","            learning_rate=self._learning_rate,\n","            embed_k=self._factors,\n","            l_w=self._l_w,\n","            weight_size=self._weight_size,\n","            n_layers=self._n_layers,\n","            message_dropout=self._message_dropout,\n","            random_seed=self._seed\n","        )\n","\n","    # method to perform node dropout\n","    def sparse_dropout(self, x, rate, noise_shape):\n","        random_tensor = 1 - rate\n","        random_tensor += torch.rand(noise_shape).to(x.device())\n","        dropout_mask = torch.floor(random_tensor).type(torch.bool)\n","        i = self.adj.to_torch_sparse_coo_tensor().coalesce().indices()\n","        v = self.adj.to_torch_sparse_coo_tensor().coalesce().values()\n","\n","        i = i[:, dropout_mask]\n","        v = v[dropout_mask]\n","\n","        out = SparseTensor(row=i[0],\n","                           col=i[1],\n","                           value=v * (1. / (1 - rate)),\n","                           sparse_sizes=(self._num_users + self._num_items,\n","                                         self._num_users + self._num_items))\n","        return out\n","\n","    # method to perform adjacency normalization\n","    @staticmethod\n","    def apply_norm(edge_index, add_self_loops=True):\n","        adj_t = edge_index\n","        if add_self_loops:\n","            adj_t = fill_diag(adj_t, 1.)\n","        deg = sum(adj_t, dim=1)\n","        deg_inv = deg.pow_(-1)\n","        deg_inv.masked_fill_(deg_inv == float('inf'), 0.)\n","        norm_adj_t = mul(adj_t, deg_inv.view(-1, 1))\n","        return norm_adj_t\n","    \n","    def train(self):\n","        for it in self.iterate(self._epochs):\n","            loss = 0\n","            steps = 0\n","            self._model.train()\n","\n","            # optionally run node dropout\n","            if self._node_dropout > 0:\n","                sampled_adj = self.sparse_dropout(self.adj,\n","                                                  self._node_dropout,\n","                                                  self.adj.nnz())\n","\n","            # train model\n","            n_batch = int(self._data.transactions / self._batch_size) if self._data.transactions % self._batch_size == 0 \\\n","            else int(self._data.transactions / self._batch_size) + 1\n","            with tqdm(total=n_batch, disable=not self._verbose) as t:\n","                for _ in range(n_batch):\n","                    user, pos, neg = self._sampler.step()\n","                    steps += 1\n","                    if self._node_dropout > 0:\n","                        loss += self._model.train_step((user, pos, neg), sampled_adj)\n","                    else:\n","                        loss += self._model.train_step((user, pos, neg), self.adj)\n","\n","                    if math.isnan(loss) or math.isinf(loss) or (not loss):\n","                        break\n","\n","                    t.set_postfix({'loss': f'{loss / steps:.5f}'})\n","                    t.update()\n","\n","            # run evaluation\n","            self.evaluate(it, loss / (it + 1))\n","\n","```\n"],"metadata":{"id":"9MNlT6OwthKH"}},{"cell_type":"markdown","source":["#### Model class\n","Let's take a look at the model-class for NGCF (the file is ```NGCFModel.py```):\n","\n","\n","```python\n","class NGCFModel(torch.nn.Module, ABC):\n","    def __init__(self,\n","                 num_users,\n","                 num_items,\n","                 learning_rate,\n","                 embed_k,\n","                 l_w,\n","                 weight_size,\n","                 n_layers,\n","                 message_dropout,\n","                 random_seed,\n","                 name=\"NGFC\",\n","                 **kwargs\n","                 ):\n","        super().__init__()\n","\n","        # set all seeds and deterministic behaviour\n","        random.seed(random_seed)\n","        np.random.seed(random_seed)\n","        torch.manual_seed(random_seed)\n","        torch.cuda.manual_seed(random_seed)\n","        torch.cuda.manual_seed_all(random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.use_deterministic_algorithms(True)\n","\n","        # set device\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # set all model's parameters\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.embed_k = embed_k\n","        self.learning_rate = learning_rate\n","        self.l_w = l_w\n","        self.weight_size = weight_size\n","        self.n_layers = n_layers\n","        self.message_dropout = message_dropout\n","        self.weight_size_list = [self.embed_k] + ([self.weight_size] * self.n_layers)\n","\n","        # create node embeddings\n","        self.Gu = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(self.num_users, self.embed_k)))\n","        self.Gu.to(self.device)\n","        self.Gi = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(self.num_items, self.embed_k)))\n","        self.Gi.to(self.device)\n","\n","        # create GNN\n","        propagation_network_list = []\n","        self.dropout_layers = []\n","        for layer in range(self.n_layers):\n","            propagation_network_list.append((NGCFLayer(self.weight_size_list[layer],\n","                                                       self.weight_size_list[layer + 1]), 'x, edge_index -> x'))\n","            self.dropout_layers.append(torch.nn.Dropout(p=self.message_dropout))\n","        self.propagation_network = torch_geometric.nn.Sequential('x, edge_index', propagation_network_list)\n","        self.propagation_network.to(self.device)\n","\n","        # instantiate optimizer\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","\n","    # train step batch-by-batch\n","    def train_step(self, batch, adj):\n","\n","        # run message-passing on the whole GNN\n","        gu, gi = self.propagate_embeddings(adj)\n","\n","        # run MF\n","        user, pos, neg = batch\n","        xu_pos, gamma_u, gamma_i_pos = self.forward(inputs=(gu[user], gi[pos]))\n","        xu_neg, _, gamma_i_neg = self.forward(inputs=(gu[user], gi[neg]))\n","\n","        # compute loss\n","        maxi = torch.nn.LogSigmoid()(xu_pos - xu_neg)\n","        mf_loss = -1 * torch.mean(maxi)\n","        reg_loss = self.l_w * (1 / 2) * (torch.norm(gu[user]) ** 2\n","                                         + torch.norm(gi[pos]) ** 2\n","                                         + torch.norm(gi[neg]) ** 2) / len(user)\n","        mf_loss += reg_loss\n","\n","        # backward propagation\n","        self.optimizer.zero_grad()\n","        mf_loss.backward()\n","        self.optimizer.step()\n","\n","        # return batch loss\n","        return mf_loss.detach().cpu().numpy()\n","    \n","    # method to run message-passing\n","    def propagate_embeddings(self, adj):\n","\n","        # create a unique embedding representation for all users $\\mathbf{E}_u^{(0)}$ and items $\\mathbf{E}_i^{(0)}$\n","        ego_embeddings = torch.cat((self.Gu.to(self.device), self.Gi.to(self.device)), 0)\n","        all_embeddings = [ego_embeddings]\n","        embedding_idx = 0\n","\n","        # run message-passing\n","        for layer in range(self.n_layers):\n","            all_embeddings += [torch.nn.functional.normalize(self.dropout_layers[embedding_idx](list(\n","                self.propagation_network.children()\n","            )[layer](all_embeddings[embedding_idx].to(self.device), adj.to(self.device))), p=2, dim=1)]\n","            embedding_idx += 1\n","\n","        # aggregate all embedding representations from each layer $[\\mathbf{E}^{(0)}, \\mathbf{E}^{(1)}, \\dots, \\mathbf{E}^{(L - 1)}]$\n","        all_embeddings = torch.cat(all_embeddings, 1)\n","\n","        # get the final embedding representation\n","        gu, gi = torch.split(all_embeddings, [self.num_users, self.num_items], 0)\n","\n","        return gu, gi\n","\n","    # method to run MF\n","    def forward(self, inputs, **kwargs):\n","          gu, gi = inputs\n","          gamma_u = torch.squeeze(gu).to(self.device)\n","          gamma_i = torch.squeeze(gi).to(self.device)\n","\n","          xui = torch.sum(gamma_u * gamma_i, 1)\n","\n","          return xui, gamma_u, gamma_i\n","```\n","\n"],"metadata":{"id":"ZHp0BoMBtqTw"}},{"cell_type":"markdown","source":["#### Layer class\n","\n","Let's recall the message-passing formulation for NGCF:\n","\n","$$\\mathbf{E}^{(l)} = \\text{LeakyReLU}\\Big(\\underbrace{\\underbrace{\\hat{A}\\mathbf{E}^{(l-1)}}_{(1)}\\mathbf{W}_1^{(l)}}_{(2)} + \\overbrace{\\underbrace{\\underbrace{\\hat{A}\\mathbf{E}^{(l-1)}}_{(1)}\\odot\\mathbf{E}^{(l-1)}}_{(3)}\\mathbf{W}_2^{(l)}}^{(4)}\\Big),$$\n","\n","where $\\hat{A}$ is the normalized adjacency matrix. The normalization is handled in the base-class (see above).\n","\n","Let's take a look at the layer-class for NGCF (the file is ```NGCFLayer.py```):\n","\n","```python\n","import torch\n","from torch_geometric.nn import MessagePassing\n","from torch_sparse import matmul\n","\n","# this class extends the PyG's MessagePassing class\n","class NGCFLayer(MessagePassing):\n","    def __init__(self, in_dim, out_dim):\n","        super(NGCFLayer, self).__init__(aggr='add')\n","        self.W1 = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(in_dim, out_dim)))\n","        self.b1 = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1, out_dim)))\n","        self.W2 = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(in_dim, out_dim)))\n","        self.b2 = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty(1, out_dim)))\n","        self.leaky_relu = torch.nn.LeakyReLU(negative_slope=0.2)\n","\n","    def init_weights(self):\n","        torch.nn.init.xavier_uniform_(self.lin1.weight)\n","        torch.nn.init.xavier_uniform_(self.lin1.bias.unsqueeze(0))\n","        torch.nn.init.xavier_uniform_(self.lin2.weight)\n","        torch.nn.init.xavier_uniform_(self.lin2.bias.unsqueeze(0))\n","\n","    def forward(self, x, edge_index):\n","        return self.propagate(edge_index, x=x)\n","\n","    # method to perform message-passing\n","    def message_and_aggregate(self, adj_t, x):\n","\n","        # we calculate (1)\n","        side_embeddings = matmul(adj_t, x, reduce=self.aggr)\n","\n","        # we calculate (2)\n","        first_addendum = torch.matmul(side_embeddings, self.W1)\n","\n","        # we calculate (3)\n","        o_dot = torch.mul(side_embeddings, x)\n","\n","        # we calculate (4)\n","        second_addendum = torch.matmul(o_dot, self.W2)\n","\n","        # note that the authors add the bias term in their original code, that is not reported in the\n","        return self.leaky_relu(first_addendum + self.b1 + second_addendum + self.b2)\n","```\n","\n"],"metadata":{"id":"YyzX7QqOxyFg"}},{"cell_type":"markdown","source":["### Light Graph Convolutional Network (LightGCN)\n","\n","Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, Meng Wang:\n","_LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation_. SIGIR 2020: 639-648\n","\n","<div>\n","  <img src=\"https://recbole.io/docs/_images/lightgcn.png\" alt=\"NGCF\" width=\"500\">\n","</div>\n","\n","\\[[**paper**](https://arxiv.org/abs/2002.02126)\\]\\[[**code**](https://github.com/gusye1234/LightGCN-PyTorch)\\]"],"metadata":{"id":"2pqte9dekXHh"}},{"cell_type":"markdown","source":["#### Base class\n","\n","Let's take a look at the base-class for LightGCN (the file is ```LightGCN.py```):\n","\n","```python\n","class LightGCN(RecMixin, BaseRecommenderModel):\n","    r\"\"\"\n","    LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation\n","\n","    For further details, please refer to the `paper <https://dl.acm.org/doi/10.1145/3397271.3401063>`_\n","    \"\"\"\n","    @init_charger\n","    def __init__(self, data, config, params, *args, **kwargs):\n","\n","        # parameters list for LightGCN\n","        self._params_list = [\n","            (\"_learning_rate\", \"lr\", \"lr\", 0.0005, float, None),\n","            (\"_factors\", \"factors\", \"factors\", 64, int, None),\n","            (\"_l_w\", \"l_w\", \"l_w\", 0.01, float, None),\n","            (\"_n_layers\", \"n_layers\", \"n_layers\", 1, int, None),\n","            (\"_normalize\", \"normalize\", \"normalize\", True, bool, None)\n","        ]\n","        self.autoset_params()\n","\n","        # create the sampler for BPR\n","        self._sampler = Sampler(self._data.i_train_dict, seed=self._seed)\n","\n","        # instantiate the adjacency matrix\n","        row, col = data.sp_i_train.nonzero()\n","        col = [c + self._num_users for c in col]\n","        edge_index = np.array([row, col])\n","        edge_index = torch.tensor(edge_index, dtype=torch.int64)\n","        self.adj = SparseTensor(row=torch.cat([edge_index[0], edge_index[1]], dim=0),\n","                                col=torch.cat([edge_index[1], edge_index[0]], dim=0),\n","                                sparse_sizes=(self._num_users + self._num_items,\n","                                              self._num_users + self._num_items))\n","\n","        # instantiate the model\n","        self._model = LightGCNModel(\n","            num_users=self._num_users,\n","            num_items=self._num_items,\n","            learning_rate=self._learning_rate,\n","            embed_k=self._factors,\n","            l_w=self._l_w,\n","            n_layers=self._n_layers,\n","            adj=self.adj,\n","            normalize=self._normalize,\n","            random_seed=self._seed\n","        )\n","    \n","    def train(self):\n","        for it in self.iterate(self._epochs):\n","            loss = 0\n","            steps = 0\n","            self._model.train()\n","\n","            # optionally run node dropout\n","            if self._node_dropout > 0:\n","                sampled_adj = self.sparse_dropout(self.adj,\n","                                                  self._node_dropout,\n","                                                  self.adj.nnz())\n","\n","            # train model\n","            n_batch = int(self._data.transactions / self._batch_size) if self._data.transactions % self._batch_size == 0 \\\n","            else int(self._data.transactions / self._batch_size) + 1\n","            with tqdm(total=n_batch, disable=not self._verbose) as t:\n","                for batch in self._sampler.step(self._data.transactions, self._batch_size):\n","                    steps += 1\n","                    loss += self._model.train_step(batch)\n","\n","                    if math.isnan(loss) or math.isinf(loss) or (not loss):\n","                        break\n","\n","                    t.set_postfix({'loss': f'{loss / steps:.5f}'})\n","                    t.update()\n","\n","            # run evaluation\n","            self.evaluate(it, loss / (it + 1))\n","\n","```\n"],"metadata":{"id":"tT76sAYJmrVT"}},{"cell_type":"markdown","source":["#### Model class\n","Let's take a look at the model-class for LightGCN (the file is ```LightGCNModel.py```):\n","\n","\n","```python\n","class LightGCNModel(torch.nn.Module, ABC):\n","    def __init__(self,\n","                 num_users,\n","                 num_items,\n","                 learning_rate,\n","                 embed_k,\n","                 l_w,\n","                 n_layers,\n","                 adj,\n","                 normalize,\n","                 random_seed,\n","                 name=\"LightGCN\",\n","                 **kwargs\n","                 ):\n","        super().__init__()\n","\n","        # set all seeds and deterministic behaviour\n","        random.seed(random_seed)\n","        np.random.seed(random_seed)\n","        torch.manual_seed(random_seed)\n","        torch.cuda.manual_seed(random_seed)\n","        torch.cuda.manual_seed_all(random_seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.use_deterministic_algorithms(True)\n","\n","        # set device\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        # set all model's parameters\n","        self.num_users = num_users\n","        self.num_items = num_items\n","        self.embed_k = embed_k\n","        self.learning_rate = learning_rate\n","        self.l_w = l_w\n","        self.n_layers = n_layers\n","        self.weight_size_list = [self.embed_k] * (self.n_layers + 1)\n","        self.alpha = torch.tensor([1 / (k + 1) for k in range(len(self.weight_size_list))])\n","        self.adj = adj\n","        self.normalize = normalize\n","\n","        # create node embeddings\n","        self.Gu = torch.nn.Embedding(\n","            num_embeddings=self.num_users, embedding_dim=self.embed_k)\n","        self.Gi = torch.nn.Embedding(\n","            num_embeddings=self.num_items, embedding_dim=self.embed_k)\n","        torch.nn.init.normal_(self.Gu.weight, std=0.1)\n","        torch.nn.init.normal_(self.Gi.weight, std=0.1)\n","\n","        # create GNN\n","        propagation_network_list = []\n","        for _ in range(self.n_layers):\n","            propagation_network_list.append((LGConv(normalize=self.normalize), 'x, edge_index -> x'))\n","        self.propagation_network = torch_geometric.nn.Sequential('x, edge_index', propagation_network_list)\n","        self.propagation_network.to(self.device)\n","        self.softplus = torch.nn.Softplus()\n","\n","        # instantiate optimizer\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","\n","    # train step batch-by-batch\n","    def train_step(self, batch, adj):\n","\n","        # run message-passing on the whole GNN\n","        gu, gi = self.propagate_embeddings(adj)\n","\n","        # run MF\n","        user, pos, neg = batch\n","        xu_pos = self.forward(inputs=(gu[user[:, 0]], gi[pos[:, 0]]))\n","        xu_neg = self.forward(inputs=(gu[user[:, 0]], gi[neg[:, 0]]))\n","\n","        # compute loss\n","        loss = torch.mean(torch.nn.functional.softplus(xu_neg - xu_pos))\n","        reg_loss = self.l_w * (1 / 2) * (self.Gu.weight[user[:, 0]].norm(2).pow(2) +\n","                                         self.Gi.weight[pos[:, 0]].norm(2).pow(2) +\n","                                         self.Gi.weight[neg[:, 0]].norm(2).pow(2)) / float(batch[0].shape[0])\n","        loss += reg_loss\n","\n","        # backward propagation\n","        self.optimizer.zero_grad()\n","        mf_loss.backward()\n","        self.optimizer.step()\n","\n","        # return batch loss\n","        return loss.detach().cpu().numpy()\n","    \n","    # method to run message-passing\n","    def propagate_embeddings(self, adj):\n","\n","        # create a unique embedding representation for all users $\\mathbf{E}_u^{(0)}$ and items $\\mathbf{E}_i^{(0)}$\n","        ego_embeddings = torch.cat((self.Gu.to(self.device), self.Gi.to(self.device)), 0)\n","        all_embeddings = [ego_embeddings]\n","\n","        # run message-passing\n","        for layer in range(self.n_layers):\n","            all_embeddings += [list(\n","                        self.propagation_network.children()\n","                    )[layer](all_embeddings[layer].to(self.device), self.adj.to(self.device))]\n","\n","        # aggregate all embedding representations from each layer $[\\mathbf{E}^{(0)}, \\mathbf{E}^{(1)}, \\dots, \\mathbf{E}^{(L - 1)}]$\n","        # note that this aggregation is slightly different from the one reported in the paper (see in the next cell)\n","        all_embeddings = torch.mean(torch.stack(all_embeddings, 0), dim=0)\n","\n","        # get the final embedding representation\n","        gu, gi = torch.split(all_embeddings, [self.num_users, self.num_items], 0)\n","\n","        return gu, gi\n","\n","    # method to run MF\n","    def forward(self, inputs, **kwargs):\n","          gu, gi = inputs\n","          gamma_u = torch.squeeze(gu).to(self.device)\n","          gamma_i = torch.squeeze(gi).to(self.device)\n","\n","          xui = torch.sum(gamma_u * gamma_i, 1)\n","\n","          return xui, gamma_u, gamma_i\n","```\n","\n"],"metadata":{"id":"1AL4zI9gnS4H"}},{"cell_type":"markdown","source":["#### Layer class\n","\n","Let's recall the message-passing formulation for LightGCN:\n","\n","$$\\mathbf{E}^{(l)} = \\hat{A}\\mathbf{E}^{(l-1)},$$\n","\n","where $\\hat{A}$ is the normalized adjacency matrix (the class has an attribute that, when set, normalizes the adjacency matrix before running the message-passing.\n","\n","Let's take a look at the layer-class for LightGCN. In this case, the layer has been already implemented in PyG ([here](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/lg_conv.html#LGConv) for a reference):\n","\n","```python\n","from torch import Tensor\n","\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.nn.conv.gcn_conv import gcn_norm\n","from torch_geometric.typing import Adj, OptTensor, SparseTensor\n","from torch_geometric.utils import spmm\n","\n","\n","class LGConv(MessagePassing):\n","    r\"\"\"The Light Graph Convolution (LGC) operator from the `\"LightGCN:\n","    Simplifying and Powering Graph Convolution Network for Recommendation\"\n","    <https://arxiv.org/abs/2002.02126>`_ paper.\n","\n","    .. math::\n","        \\mathbf{x}^{\\prime}_i = \\sum_{j \\in \\mathcal{N}(i)}\n","        \\frac{e_{j,i}}{\\sqrt{\\deg(i)\\deg(j)}} \\mathbf{x}_j\n","\n","    Args:\n","        normalize (bool, optional): If set to :obj:`False`, output features\n","            will not be normalized via symmetric normalization.\n","            (default: :obj:`True`)\n","        **kwargs (optional): Additional arguments of\n","            :class:`torch_geometric.nn.conv.MessagePassing`.\n","\n","    Shapes:\n","        - **input:**\n","          node features :math:`(|\\mathcal{V}|, F)`,\n","          edge indices :math:`(2, |\\mathcal{E}|)`,\n","          edge weights :math:`(|\\mathcal{E}|)` *(optional)*\n","        - **output:** node features :math:`(|\\mathcal{V}|, F)`\n","    \"\"\"\n","    def __init__(self, normalize: bool = True, **kwargs):\n","        kwargs.setdefault('aggr', 'add')\n","        super().__init__(**kwargs)\n","        self.normalize = normalize\n","    def forward(self, x: Tensor, edge_index: Adj,\n","                edge_weight: OptTensor = None) -> Tensor:\n","\n","        if self.normalize and isinstance(edge_index, Tensor):\n","            out = gcn_norm(edge_index, edge_weight, x.size(self.node_dim),\n","                           add_self_loops=False, flow=self.flow, dtype=x.dtype)\n","            edge_index, edge_weight = out\n","        elif self.normalize and isinstance(edge_index, SparseTensor):\n","            edge_index = gcn_norm(edge_index, None, x.size(self.node_dim),\n","                                  add_self_loops=False, flow=self.flow,\n","                                  dtype=x.dtype)\n","\n","        # propagate_type: (x: Tensor, edge_weight: OptTensor)\n","        return self.propagate(edge_index, x=x, edge_weight=edge_weight,\n","                              size=None)\n","\n","    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n","        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n","\n","    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n","        return spmm(adj_t, x, reduce=self.aggr)\n","```\n","\n"],"metadata":{"id":"2NtgTL1DojSj"}},{"cell_type":"markdown","source":["## Dealing with reproducibility issues\n","\n","Let's verify that the results obtained with our code are fully reproducible."],"metadata":{"id":"cDNBzIm3O64t"}},{"cell_type":"markdown","source":["### Test LightGCN with reproducibility\n","\n","To do so, we first run LightGCN on a small number of epochs (i.e., 2 epochs) and visualize the results."],"metadata":{"id":"HrZjW7cqDRWb"}},{"cell_type":"code","source":["config_filename = 'hands-on-1-log_2023_reproducibility.yml'\n","config = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train.tsv',\n","      'test_path': '../data/{0}/test.tsv'\n","    },\n","    'dataset': 'gowalla',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'nDCG']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.LightGCN': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'validation_rate': 2,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'lr': 0.001,\n","        'epochs': 2,\n","        'factors': 64,\n","        'batch_size': 2048,\n","        'l_w': 1e-4,\n","        'n_layers': 3,\n","        'normalize': True,\n","        'seed': 42\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}', 'w') as file:\n","    documents = yaml.dump(config, file)\n","\n","run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"E7x2gnlwZOox"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's run it again:"],"metadata":{"id":"jZG0AktCZZto"}},{"cell_type":"code","source":["run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"eS0DLPJAZcdp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["... and again:"],"metadata":{"id":"a6rhvQKdZf_K"}},{"cell_type":"code","source":["run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"5lKuJgEdZkhB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The results are fully aligned**. You can try it yourself on your custom experimental configurations!"],"metadata":{"id":"pFmpwRUqZlh4"}},{"cell_type":"markdown","source":["### How can we seek reproducibility?\n","\n","We have three thumb rules to follow:\n","\n","1. Set all random seeds\n","2. Use deterministics operations\n","3. Set the proper environment variables"],"metadata":{"id":"1mT16JY4DZml"}},{"cell_type":"markdown","source":["#### Set all random seeds\n","\n","This is the piece of code that we use to set all random seeds:\n","\n","```python\n","random.seed(random_seed)\n","np.random.seed(random_seed)\n","torch.manual_seed(random_seed)\n","torch.cuda.manual_seed(random_seed)\n","torch.cuda.manual_seed_all(random_seed)\n","```\n","\n"],"metadata":{"id":"rFbOzhGNJiPk"}},{"cell_type":"markdown","source":["#### Use deterministics operations\n","\n","In PyTorch (as in other Python libraries), some operations may be deterministic, while other ones may have a non-deterministic behaviour.\n","\n","In some cases, PyTorch provides, for an operation, both versions. To ensure that only the deterministic one is chosen during the running, you should include the following:\n","\n","```python\n","torch.backends.cudnn.deterministic = True\n","torch.use_deterministic_algorithms(True)\n","```\n","\n","This will throw an exception if the operation has only the non-deterministic version. At this [link](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html) you may find an updated list of all non-deterministic operations in PyTorch.\n","\n"],"metadata":{"id":"JiF6-Q6sJ46F"}},{"cell_type":"markdown","source":["#### Set the proper environment variables\n","\n","As indicated the in the official [documentation](https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html), one should set the environment variable ```CUBLAS_WORKSPACE_CONFIG=:4096:8``` or ```CUBLAS_WORKSPACE_CONFIG=:16:8``` to use deterministic operations if the CUDA version is >= 10.2.\n","\n","We do this with before launching any script:\n","\n","\n","\n","```sh\n","%env CUBLAS_WORKSPACE_CONFIG=:16:8\n","```\n","\n","\n","\n"],"metadata":{"id":"NO-vXHnJK6rp"}},{"cell_type":"markdown","source":["### What about PyTorch Geometric?\n","\n","Let's take a look at the layer-class of LightGCN once again:\n","\n","```python\n","from torch import Tensor\n","\n","from torch_geometric.nn.conv import MessagePassing\n","from torch_geometric.nn.conv.gcn_conv import gcn_norm\n","from torch_geometric.typing import Adj, OptTensor, SparseTensor\n","from torch_geometric.utils import spmm\n","\n","\n","class LGConv(MessagePassing):\n","    # [...]\n","    def __init__(self, normalize: bool = True, **kwargs):\n","        kwargs.setdefault('aggr', 'add')\n","        super().__init__(**kwargs)\n","        self.normalize = normalize\n","    def forward(self, x: Tensor, edge_index: Adj,\n","                edge_weight: OptTensor = None) -> Tensor:\n","\n","        # [...]\n","        return self.propagate(edge_index, x=x, edge_weight=edge_weight,\n","                              size=None)\n","\n","    def message(self, x_j: Tensor, edge_weight: OptTensor) -> Tensor:\n","        return x_j if edge_weight is None else edge_weight.view(-1, 1) * x_j\n","\n","    def message_and_aggregate(self, adj_t: SparseTensor, x: Tensor) -> Tensor:\n","        return spmm(adj_t, x, reduce=self.aggr)\n","```\n","\n","If we go deeper in the class hierarchy, we find that the base version of the ```propagate()``` method is:\n","\n","\n","\n","```python\n","def propagate(self, edge_index: Adj, size: Size = None, **kwargs):\n","  # [...]\n","  if is_sparse(edge_index) and self.fuse and not self.explain:\n","    # [...]\n","    out = self.message_and_aggregate(edge_index, **msg_aggr_kwargs)\n","    # [...]\n","  else:  # Otherwise, run both functions in separation.\n","    # [...]\n","    out = self.message(**msg_kwargs)\n","    # [...]\n","    out = self.aggregate(out, **aggr_kwargs)\n","```\n","\n","That is, the ```propagate()``` method calls the method ```message_and_aggregate()``` when the adjacency matrix is formatted as a ```SparseTensor```, while it calls the methods ```message()``` and ```aggregate()``` when the adjacency matrix is formatted as a ```Tensor```.\n","\n","By going even deeper, we find out that the ```aggregate()``` method may call the ```scatter()``` method, which is one of those operations behaving in a non-deterministic manner.\n","\n","This is the reason why, **if we aim to reach full reproducibility, we should always format the adjacency matrix as a ```SparseTensor```**.\n","\n","\n","However, this is done when the message-passing formulation of our model may be expressed **at graph level, and not only at node level**.\n","\n","$$\\begin{align}\n","\\text{graph-level} \\; → \\; & \\mathbf{E}^{(l)} = \\hat{A}\\mathbf{E}^{(l - 1)}\\\\\n","\\text{node-level} \\; → \\; & \\mathbf{e}_u^{(l)} = \\sum_{i \\in \\mathcal{N}_u} \\frac{\\mathbf{e}_i^{(l-1)}}{\\sqrt{|\\mathcal{N}_u||\\mathcal{N}_i|}},\\quad \\forall u \\in \\mathcal{U}.\n","\\end{align}\n","$$\n","\n","Take at look at this [link](https://github.com/pytorch/pytorch/issues/50469) and this other [link](https://pytorch-geometric.readthedocs.io/en/latest/notes/sparse_tensor.html) for further references."],"metadata":{"id":"7ybawZFsL6sO"}},{"cell_type":"markdown","source":["### Test LightGCN without reproducibility\n","\n","Here we test a non-reproducible version of LightGCN, where the adjacency matrix is represented with a ```Tensor```:\n","\n","```python\n","# Base class\n","class LightGCNNoRepr(RecMixin, BaseRecommenderModel):\n","  @init_charger\n","    def __init__(self, data, config, params, *args, **kwargs):\n","        ######################################\n","\n","        self._params_list = [\n","            (\"_learning_rate\", \"lr\", \"lr\", 0.0005, float, None),\n","            (\"_factors\", \"factors\", \"factors\", 64, int, None),\n","            (\"_l_w\", \"l_w\", \"l_w\", 0.01, float, None),\n","            (\"_n_layers\", \"n_layers\", \"n_layers\", 1, int, None),\n","            (\"_normalize\", \"normalize\", \"normalize\", True, bool, None)\n","        ]\n","        self.autoset_params()\n","        # [...]\n","        row, col = data.sp_i_train.nonzero()\n","        col = [c + self._num_users for c in col]\n","        edge_index = np.array([row, col])\n","\n","        # we use the tensor version instead of the sparse tensor one from above\n","        edge_index = torch.tensor(edge_index, dtype=torch.int64)\n","\n","        self._model = LightGCNNoReprModel(\n","            num_users=self._num_users,\n","            num_items=self._num_items,\n","            learning_rate=self._learning_rate,\n","            embed_k=self._factors,\n","            l_w=self._l_w,\n","            n_layers=self._n_layers,\n","            edge_index=edge_index,\n","            normalize=self._normalize,\n","            random_seed=self._seed\n","        )\n","        # [...]\n","```\n","\n","\n","```python\n","# Model class\n","class LightGCNNoReprModel(torch.nn.Module, ABC):\n","  # [...]\n","      def propagate_embeddings(self, evaluate=False):\n","        ego_embeddings = torch.cat((self.Gu.weight.to(self.device), self.Gi.weight.to(self.device)), 0)\n","        all_embeddings = [ego_embeddings]\n","\n","        for layer in range(0, self.n_layers):\n","            if evaluate:\n","                self.propagation_network.eval()\n","                with torch.no_grad():\n","                    all_embeddings += [list(\n","                        self.propagation_network.children()\n","                    )[layer](all_embeddings[layer].to(self.device), self.edge_index.to(self.device))]\n","            else:\n","                all_embeddings += [list(\n","                    self.propagation_network.children()\n","                )[layer](all_embeddings[layer].to(self.device), self.edge_index.to(self.device))]\n","\n","        if evaluate:\n","            self.propagation_network.train()\n","\n","        all_embeddings = torch.mean(torch.stack(all_embeddings, 0), dim=0)\n","        gu, gi = torch.split(all_embeddings, [self.num_users, self.num_items], 0)\n","\n","        return gu, gi\n","  # [...]\n","```\n","\n","We run a short experiment now:\n"],"metadata":{"id":"3wMJIctADism"}},{"cell_type":"code","source":["config = {\n","  'experiment': {\n","    'backend': 'pytorch',\n","    'data_config': {\n","      'strategy': 'fixed',\n","      'train_path': '../data/{0}/train.tsv',\n","      'test_path': '../data/{0}/test.tsv'\n","    },\n","    'dataset': 'gowalla',\n","    'top_k': 20,\n","    'evaluation': {\n","      'cutoffs': [20],\n","      'simple_metrics': ['Recall', 'nDCG']\n","    },\n","    'gpu': 0,\n","    'external_models_path': '../external/models/__init__.py',\n","    'models': {\n","      'external.LightGCNNoRepr': {\n","        'meta': {\n","          'hyper_opt_alg': 'grid',\n","          'verbose': True,\n","          'save_weights': False,\n","          'validation_rate': 2,\n","          'validation_metric': 'Recall@20',\n","          'restore': False\n","        },\n","        'lr': 0.001,\n","        'epochs': 2,\n","        'factors': 64,\n","        'batch_size': 2048,\n","        'l_w': 1e-4,\n","        'n_layers': 3,\n","        'normalize': True,\n","        'seed': 42\n","      }\n","    }\n","  }\n","}\n","\n","with open(f'config_files/{config_filename}', 'w') as file:\n","    documents = yaml.dump(config, file)\n","\n","run_experiment(f\"config_files/{config_filename}\")"],"metadata":{"id":"zZsr8h0GVouE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**The results are different!**"],"metadata":{"id":"02bMHoV8V7o9"}},{"cell_type":"markdown","source":["## Cite us\n","\n","This notebook is heavily dependent on [Elliot](https://github.com/sisinflab/elliot), our framework for rigorous and reproducible recommender systems evaluation. If you use this in your works, please consider to cite us:\n","\n","\n","\n","```\n","@inproceedings{DBLP:conf/sigir/AnelliBFMMPDN21,\n","  author       = {Vito Walter Anelli and\n","                  Alejandro Bellog{\\'{\\i}}n and\n","                  Antonio Ferrara and\n","                  Daniele Malitesta and\n","                  Felice Antonio Merra and\n","                  Claudio Pomo and\n","                  Francesco Maria Donini and\n","                  Tommaso Di Noia},\n","  title        = {Elliot: {A} Comprehensive and Rigorous Framework for Reproducible\n","                  Recommender Systems Evaluation},\n","  booktitle    = {{SIGIR}},\n","  pages        = {2405--2414},\n","  publisher    = {{ACM}},\n","  year         = {2021}\n","}\n","```\n","\n","\n","\n","```\n","@inproceedings{DBLP:conf/recsys/AnelliMPBSN23,\n","  author       = {Vito Walter Anelli and\n","                  Daniele Malitesta and\n","                  Claudio Pomo and\n","                  Alejandro Bellog{\\'{\\i}}n and\n","                  Eugenio Di Sciascio and\n","                  Tommaso Di Noia},\n","  title        = {Challenging the Myth of Graph Collaborative Filtering: a Reasoned\n","                  and Reproducibility-driven Analysis},\n","  booktitle    = {RecSys},\n","  pages        = {350--361},\n","  publisher    = {{ACM}},\n","  year         = {2023}\n","}\n","```\n","\n","\n","\n","```\n","@inproceedings{DBLP:conf/um/MalitestaPANF23,\n","  author       = {Daniele Malitesta and\n","                  Claudio Pomo and\n","                  Vito Walter Anelli and\n","                  Tommaso Di Noia and\n","                  Antonio Ferrara},\n","  title        = {An Out-of-the-Box Application for Reproducible Graph Collaborative\n","                  Filtering extending the Elliot Framework},\n","  booktitle    = {{UMAP} (Adjunct Publication)},\n","  pages        = {12--15},\n","  publisher    = {{ACM}},\n","  year         = {2023}\n","}\n","```\n"],"metadata":{"id":"TSm0PwiIkecu"}}]}